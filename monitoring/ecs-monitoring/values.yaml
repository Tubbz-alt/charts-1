#
# Copyright Â© [2020] Dell Inc. or its subsidiaries.
# All Rights Reserved.
#
# This software contains the intellectual property of Dell Inc.
# or is licensed to Dell Inc. from third parties. Use of this
# software and the intellectual property contained therein is expressly
# limited to the terms and conditions of the License Agreement under which
# it is provided by or on behalf of Dell Inc. or its subsidiaries.
#
#

## Default values.yaml for Monitoring
## This is a YAML-formatted file.
---
global:
  rsyslog_uds_dir: "/var/run/rsyslog"
  rsyslog_uds_filename: "rsyslog.socket"
  use_vault_for_ssl: false
  tls_enabled: false
  telegraf_tls_enabled: false
  # communication_scheme defined separately from tls_enabled as found no way to make conditional set in values.yaml
  communication_scheme: http
  internal_dns: kube-dns.kube-system.svc.cluster.local
  rsyslog_enabled: true
  performanceProfile: Small
  objectscale_release_name: objectscale-manager
  monitoring_tag: 3.7.0.0-1059.c386f5b7
  # add labels and annotations for logging injection
  logging_injection_enabled: true

#  objectscale_ variables are used to find objectscale services from ecs-cluster charts
#
#  objectscale_release_name: "objectscale"
#  objectscale_namespace: "objectscale"

cert:
  enabled: false
grafana:
  config:
    dashboards_dir: "dashboards/ecs"
    home_dashboard: "ObjectStore Overview"
    reverse_proxy:
      enabled: true
      protocol: "http"
      domain: "localhost"
      subpath: "grafana/{{ .Release.Namespace }}/{{ .Release.Name }}"
    # accesses to all customer dashboards
    users:
      - username: emcmonitor
        password: ChangeMe
      - username: emcservice
        password: ChangeMe
    # no accesses to all dashboards
    limitedUsers:
      - username: root
        password: ChangeMe
throttler:
  config:
    cq_dir: "cq/ecs"
    alerter:
      enabled: false
influxdb:
  persistence:
    storageClassName: "dellemc-objectscale-highly-available"
telegraf:
  config:
    influxdb_inputs_enabled: false
    # Send cq results from objectstore to region level objectscale monitoring
    ext_outputs:
      - influxdb:
          urls: ['http://{{ .Values.global.objectscale_release_name }}-telegraf.{{ include "common-lib.rsyslog_svc_namespace" . }}.svc.cluster.local:11002']  # yamllint disable-line rule:line-length
          insecure_skip_verify: true
          skip_database_creation: true
          timeout: "30s"
          namepass:
            - "cq_performance_transaction"
            - "cq_performance_throughput"
            - "cq_performance_error"
            - "cq_performance_latency"
            - "cq_capacity_vdc"
            - "cq_recover_status_summary"
            - "cq_node_rebalancing_summary"
            - "cq_capacity_ec"
            - "cq_capacity_ec_rate"
            - "cq_disk_bandwidth"
            - "cq_gc_data"
            - "cq_capacity_used_elements"
            - "cq_hardware_health_nodes_counter"
            - "cq_hardware_health_disks_counter"
          namedrop: ["cq_*_metadata"]
    outputs:
      - influxdb:
          insecure_skip_verify: true
          skip_database_creation: true
          timeout: "30s"
          database: "monitoring_main"
          retention_policy: "default"
          tagexclude: ["vdc", "storage_pool", "cluster", "release_name"]
          fielddrop: ["*Timestamp"]
          namepass:
            - "*_IO_Statistics_data_*"
            - "cm_BTREE_GC_Statistics"
            - "cm_Chunk_Statistics"
            - "cm_EC_Statistics"
            - "cm_REPO_GC_Statistics"
            - "sr_REPO_GC_Statistics"
            - "cm_Recover_Statistics"
            - "cm_Rebalance_Statistics"
            - "ssm_sstable_SSTable_SS"
            - "ssm_sstable_SSTable_SS_partitions_PD"
            - "ssm_sstable_SSTable_SS_partitions_PD_status"
            - "statDataHead_performance_internal_*"
            - "blob_SSDReadCache_Stats"
          tagpasscoordinated:
            limit: 25
            namespace: ["*"]
      - influxdb:
          insecure_skip_verify: true
          skip_database_creation: true
          timeout: "30s"
          database: "monitoring_op"
          retention_policy: "default"
          tagexclude: ["vdc", "storage_pool", "cluster", "release_name"]
          fielddrop: ["*Timestamp"]
          namepass:
            - "dtquery_dt_status"
            - "dtquery_dt_status_detailed_type"
            - "dtquery_dt_dist_dt_node_id_type"
            - "dtquery_dt_dist_host_dt_node_id"
            - "dtquery_dt_dist_type_type"
            - "*_stat_client_performance"
            - "vnestStat_btree"
            - "sr_JournalParser_GC_RG_DT"
            - "sr_ObjectGC_CAS_RG"
            - "tsdb_telegraf_internal_gather"
            - "tsdb_telegraf_internal_write"
            - "disk"
            - "procstat"
      - influxdb:
          insecure_skip_verify: true
          skip_database_creation: true
          timeout: "30s"
          database: "monitoring_last"
          retention_policy: "default"
          tagexclude: ["vdc", "storage_pool", "cluster", "release_name"]
          fielddrop: ["*Timestamp"]
          namepass:
            - "vnestStat_performance_*"
            - "vnestStat_membership_*"
            - "*_Process_status"
            - "resource_EKM_EKMServer"
            - "eventsvc_Alerts_DT"
            - "sr_ObjectGC_CAS_BUCKET_DT_Blob"
            - "sr_ObjectGC_CAS_BUCKET_DT_Reflection"
            - "dtquery_cmf"
            - "mm_topn_bucket_by_obj_size_place"
            - "mm_topn_bucket_by_obj_count_place"
      - influxdb:
          insecure_skip_verify: true
          skip_database_creation: true
          timeout: "30s"
          database: "monitoring_vdc"
          retention_policy: "default"
          # host and node_id are removed because CQs from several nodes can write same data (even if not simultaneously)
          tagexclude: ["host", "node_id", "vdc", "storage_pool", "cluster", "release_name"]
          namepass: ["cq_*"]
    inputs:
      # Influx HTTP write listener
      - influxdb_listener:
          ## Address and port to host HTTP listener on
          # redefine to 11102 when tls is enabled
          service_address: ":11002"
          ## maximum duration before timing out read of the request
          read_timeout: "30s"
          ## maximum duration before timing out write of the response
          write_timeout: "30s"
      # Collect statistics about itself
      - internal:
          interval: "300s"
          collect_memstats: true
          name_prefix: "tsdb_telegraf_"
          tags:
            tag: "system"
            host: "$HOSTNAME"
    processors:
      #
      # Drop 'host' & 'node_id' tags to reduce cardinality
      # * for DT-related metrics
      # * for cluster-wide metrics which may be reported
      #     from different nodes (when master moves)
      #
      # It does not matter which node reports a DT / cluster-wide metric.
      # It matters when a metric is reported.
      #
      - override:
          namepass:
            - "dtquery_dt_status"
            - "dtquery_dt_status_detailed_type"
            - "dtquery_dt_dist_dt_node_id_type"
            - "dtquery_dt_dist_host_dt_node_id"
            - "dtquery_dt_dist_type_type"
            - "dtquery_cmf"
            - "sr_JournalParser_GC_RG_DT"
            - "sr_ObjectGC_CAS_RG"
            - "resource_EKM_EKMServer"
            - "eventsvc_Alerts_DT"
            - "sr_ObjectGC_CAS_BUCKET_DT_Blob"
            - "sr_ObjectGC_CAS_BUCKET_DT_Reflection"
            - "ssm_sstable_SSTable_SS*"
            - "mm_topn_bucket_by_obj_size_place"
            - "mm_topn_bucket_by_obj_count_place"
          tagexclude: ["host", "node_id", "ip"]
      - rename:
          namepass: ["filestat"]
          replace:
            field: "size_bytes"
            dest: "size_bytes_i"
telegraf-prom:
  serviceAccount:
    create: true
  rbac:
    # Specifies whether RBAC resources should be created
    create: true
    # Create only for the release namespace or cluster wide (Role vs ClusterRole)
    clusterWide: false
    # Rules for the created rule
    rules:
      # When using the prometheus input to scrape all pods you need extra rules set to the ClusterRole to be
      # able to scan the pods for scraping labels. The following rules have been taken from:
      # https://github.com/helm/charts/blob/master/stable/prometheus/templates/server-clusterrole.yaml#L8-L46
      - apiGroups:
          - ""
        resources:
          - pods
        verbs:
          - get
          - list
          - watch
  replicaCount: 1
  config:
    outputs:
      - influxdb:
          insecure_skip_verify: true
          skip_database_creation: true
          timeout: "30s"
          database: "monitoring_op"
          retention_policy: "default"
          tagexclude: ["vdc", "storage_pool", "cluster", "release_name"]
          namepass:
            - "tsdb_fluxd_http_api_request_duration_seconds"
            - "tsdb_fluxd_http_api_requests_total"
            - "tsdb_fluxd_host_state_change_total"
            - "tsdb_fluxd_host_state"
            - "tsdb_fluxd_host_selected"
            - "tsdb_fluxd_host_selection_failed"
            - "tsdb_fluxd_hl_*"
            - "tsdb_fluxd_query_control_executing_duration_seconds"
            - "tsdb_fluxd_http_lb_request_forwarded"
            - "tsdb_fluxd_query_control_panics_total"
            - "tsdb_influxdb"
            - "tsdb_influxdb_database"
            - "tsdb_influxdb_queryExecutor"
            - "tsdb_telegraf_internal_gather"
            - "tsdb_telegraf_internal_write"
            - "cquerier_*"
    inputs:
      # Collect statistics about itself
      - internal:
          interval: "$COLLECTION_INTERVAL"
          collect_memstats: true
          name_prefix: "tsdb_telegraf_"
          tags:
            tag: "system"
            host: "$HOSTNAME"
      - prometheus:
          interval: "$COLLECTION_INTERVAL"
          name_prefix: "tsdb_fluxd_"
          bearer_token: "/var/run/secrets/kubernetes.io/serviceaccount/token"
          monitor_kubernetes_pods: true
          monitor_kubernetes_pods_namespace: "{{ .Release.Namespace }}"
          kubernetes_label_selector: "app.kubernetes.io/name={{ .Release.Name }}-fluxd"
          insecure_skip_verify: true
          tags:
            tag: "system"
      - prometheus:
          interval: "$COLLECTION_INTERVAL"
          kubernetes_services:
            - "{{ .Values.global.communication_scheme }}://{{ .Release.Name }}-throttler.{{ .Release.Namespace }}.svc.cluster.local:8094/metrics"
          monitor_kubernetes_pods: false
          insecure_skip_verify: true
          tags:
            tag: "system"
            host: "throttler"
    processors:
      #
      # Drop 'host' & 'node_id' tags to reduce cardinality
      # * for DT-related metrics
      # * for cluster-wide metrics which may be reported
      #     from different nodes (when master moves)
      #
      - override:
          namepass:
            - "tsdb_influxdb_httpd"
          tagexclude: ["node_id", "ip"]
